#!/usr/bin/env python3
import socket
import sys
import time

# Our data structures. Dict to store sites we've seen and an array to provide a fresh url we previously found.
links_to_parse = {}
queue = []
flags_found = []

# Searches given html for one of 5 flags.
def find_flags(html):
    stuff = html.split('\n')
    for line in stuff:
        if "secret_flag" in line:
            flags_found.append(line)
            print(line)

# This grabs links on the html we are feeding it if it's not already in the dict.
def grab_links(html):
  try:
    stuff = html.split('\n')
  except:
    print(html)
    sys.exit("tuple found")
  for line in stuff:
      if "/fakebook/" in line:
        temp = line.split('"')
        if len(temp) > 1:
            if temp[1] not in links_to_parse:
              links_to_parse[temp[1]] = False
              queue.append(temp[1])

# Generic GET request. If not the first time, then add the csrftoken and sessionID.
def sendGET(connection, url, loginHuh = False, csrfToken = None, sessionID = None):
    # Construct the GET request.
    get_req = (f"GET " + url + " HTTP/1.1\r\n"
    "Host: webcrawler-site.ccs.neu.edu\r\n")
    if not loginHuh:
        get_req += "Cookie: csrftoken=" + csrfToken + "; sessionid=" + sessionID + "\r\n"
    get_req += "\r\n"

    # Encode and send the previosuly constructed GET request.
    get_req = get_req.encode('ascii')
    connection.sendall(get_req) 
    # Return the response we get from the GET request.
    response = bytes.decode(connection.recv(4096))
    return response

# Abstract out the connection method.
def connect():
  # Setting up the connection.
  HOST = 'webcrawler-site.ccs.neu.edu'
  PORT = 80
  connection = socket.socket( socket.AF_INET, socket.SOCK_STREAM )
  connection.connect((HOST, PORT))
  return connection

# Method used to retrieeve the log in cookie that will help us stay logged in.
def login():
  # Setting up the connection.
  #HOST = 'webcrawler-site.ccs.neu.edu'
  #PORT = 80
  #connection = socket.socket( socket.AF_INET, socket.SOCK_STREAM)
  #connection.connect(( HOST, PORT ))
  connection = connect()
  response = ""
  # Retrieve the GET response and pull both csrf tokens from the login page.
  while "csrfmiddlewaretoken" not in response or "csrftoken" not in response:
    response = sendGET(connection, "/accounts/login/", True)
  # Find csrftoken lcoated in header.
  csrf1index = response.index("csrftoken")
  # Find csrftoken in body.
  csrf2index = response.index("csrfmiddlewaretoken")
  # CSRFToken pulled from header of initial GET request.
  csrfToken1 = response[csrf1index + 10 : csrf1index + 74]
  # CSRFToken pulled from the hidden csrfmiddlewaretoken field in body of initial GET request.
  csrfToken2 = response[csrf2index + 28 : csrf2index + 92]

  # Forge the POST request with the current csrf token to sign into fakebook.
  usernamePass = "username=" + sys.argv[1] + "&password=" + sys.argv[2]
  payload = "" + usernamePass + "&csrfmiddlewaretoken=" + csrfToken2 + "&next=\r\n"
  POST_REQUEST = (f"POST /accounts/login/ HTTP/1.1\r\n"
  "Host: webcrawler-site.ccs.neu.edu\r\n"
  "Accept-Encoding: gzip, deflate\r\n"
  "Referer: http://webcrawler-site.ccs.neu.edu/accounts/login/\r\n"
  "Content-Type: application/x-www-form-urlencoded\r\n"
  "Content-Length: " + str(len(payload)) + "\r\n"
  "Origin: http://webcrawler-site.ccs.neu.edu\r\n"
  "Connection: keep-alive\r\n"
  "Cookie: csrftoken=" + csrfToken1 + "\r\n"
  "Upgrade-Insecure-Requests: 1\r\n"
  "\r\n"
  + payload + "\r\n")
  POST_REQUEST = POST_REQUEST.encode('ascii')

  postResponse = ""
  # Send the Post request.
  while "csrftoken" not in postResponse or "sessionid" not in postResponse:
    connection.sendall(POST_REQUEST)
    try:
      postResponse = bytes.decode(connection.recv(4096))
    except:
      postResponse = ""
  # Give me the cookie and the sessionID.
  cookieIndex = postResponse.index("csrftoken")
  sessionIDIndex = postResponse.index("sessionid")
  cookie = postResponse[cookieIndex + 10 : cookieIndex + 74]
  sessionID = postResponse[sessionIDIndex + 10 : sessionIDIndex + 42]
  return cookie, sessionID, connection
 
# Iterate through a built up list of urls in a queue and update them in a dict when they get searched.
def iterate_through(connection, html, cookie, sessionID):
    # Do the initial html functions on our home page.
    grab_links(html)
    find_flags(html)
    curr_conn = connection
    curr_cookie = cookie
    curr_sessionID = sessionID

    # While we still have flags to find and links to explore in our queue.
    #c = 0
    while len(queue) > 0:
      curr_in_queue = queue[0]
      # Send GET request to pull down the html for the current url.
      currhtml = sendGET(curr_conn, curr_in_queue, False, curr_cookie, curr_sessionID)
      if len(currhtml) == 0:
          print(curr_in_queue)
          curr_conn.close()
          time.sleep(1)
          curr_conn = connect()
          currhtml = sendGET(curr_conn, curr_in_queue, False, curr_cookie, curr_sessionID)
          print(str(len(currhtml)))
          #c = 0
      #if c == 20:
          #curr_conn.close()
          #curr_conn = connect()
          #curr_cookie, curr_sessionID, curr_conn = login()
          #currhtml = sendGET(curr_conn, curr_in_queue, False, curr_cookie, curr_sessionID)
          #print(curr_in_queue, str(len(currhtml)))
      #c += 1
      # Search the new html page for new links.
      grab_links(currhtml)
      # Search the new html page for flags.
      find_flags(currhtml)
      # Update our data structures.
      links_to_parse[curr_in_queue] = True
      queue.pop(0)
      #print(links_to_parse)
      #print(queue)

# Main method for this webcrawler. 
# Get cookie, start searching for links and flags, maintain queue of valid links left to explore, print out found flags.
def main():
  cookie, sessionID, connection = login()
  initialHTML = ""
  while "/fakebook/" not in initialHTML:
      initialHTML = sendGET(connection, "/fakebook/", False, cookie, sessionID)
  iterate_through(connection, initialHTML, cookie, sessionID)
  #while len(queue) > 0:
      #cookie, sessionID, connection = login()
      #iterate_through(connection, queue[0], cookie, sessionID)
  print(links_to_parse)
  print("Flags found:")
  for flag in flags_found:
      print(flag)
  
if __name__ == "__main__":
  # Command line specification enforcing.
  if not(len(sys.argv) == 3):
    sys.exit("Usage: ./webcrawler [username] [password]")
  # Call the main method.
  main()
