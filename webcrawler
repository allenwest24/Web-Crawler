#!/usr/bin/env python3
import socket
import sys
import time

# Our data structures. Dict to store sites we've seen and an array to provide a fresh url we previously found.
links_to_parse = {}
queue = []
flags_found = 0
pages_seen = 0
chunked = False
chunked_off = ""
body_chunk1 = ""
body_chunked_huh = False
prev_size = 0
flags = []

# Searches given html for one of 5 flags.
def find_flags(html):
    global flags_found
    stuff = html.split('\n')
    for line in stuff:
        if "secret_flag" in line:
            index = line.index("FLAG:")
            if line[index + 6 : index + 70] not in flags:
              flags.append(line[index + 6 : index + 70])
              print(line[index + 6 : index + 70])
              flags_found += 1
            if flags_found == 5:
                sys.exit()

# This grabs links on the html we are feeding it if it's not already in the dict.
def grab_links(html):
  global chunked
  global chunked_off
  stuff = html.split('\n')
  if chunked:
      tempforchunk = stuff[0].split('"')
      chunked_off += tempforchunk[0]
      if chunked_off not in links_to_parse and "/fakebook/" in chunked_off and chunked_off[len(chunked_off) - 1] == '/':
        links_to_parse[chunked_off] = False
        queue.append(chunked_off)
      chunked = False
      chunked_off = ""
  for line in stuff:
      if "/f" in line:#akebook/" in line:
        temp = line.split('"')
        # If not chunked in the middle of the link.
        if len(temp) == 3:
            if temp[1] not in links_to_parse:
              links_to_parse[temp[1]] = False
              queue.append(temp[1])
        # If chunked in the middle of a link.
        if len(temp) == 2:
            chunked = True
            if body_chunked_huh:
              chunked_off = temp[1]
            else:
              chunked_off = temp[1]
      if "Location" in line:
          tmp = ""
          curr = ""
          ii = line.index("Location") + 10
          while curr != '\r':
              curr = line[ii]
              tmp += curr
              ii += 1
          if tmp not in links_to_parse:
              links_to_parse[tmp] = False
              queue.append(tmp)
      if "HTTP/1.1 5" in line:# or "HTTP/1.1 4":
          queue.append(html)
          print(html)

# Generic GET request. If not the first time, then add the csrftoken and sessionID.
def sendGET(connection, url, loginHuh = False, csrfToken = None, sessionID = None):
    # Construct the GET request.
    get_req = (f"GET " + url + " HTTP/1.1\r\n"
    "Host: webcrawler-site.ccs.neu.edu\r\n")
    if not loginHuh:
        get_req += "Cookie: csrftoken=" + csrfToken + "; sessionid=" + sessionID + "\r\n"
    get_req += "\r\n"

    # Encode and send the previosuly constructed GET request.
    get_req = get_req.encode('ascii')
    connection.sendall(get_req) 
    # Return the response we get from the GET request.
    response = bytes.decode(connection.recv(8192))
    return response

# Abstract out the connection method.
def connect():
  # Setting up the connection.
  HOST = 'webcrawler-site.ccs.neu.edu'
  PORT = 80
  connection = socket.socket( socket.AF_INET, socket.SOCK_STREAM )
  try:
    connection.connect((HOST, PORT))
  except:
    sys.exit("Error: Connection failed. Check VPN.")
  return connection

# Method used to retrieeve the log in cookie that will help us stay logged in.
def login():
  # Setting up the connection.
  connection = connect()

  response = ""
  # Retrieve the GET response and pull both csrf tokens from the login page.
  while "csrfmiddlewaretoken" not in response or "csrftoken" not in response:
    response = sendGET(connection, "/accounts/login/", True)
  # Find csrftoken lcoated in header.
  csrf1index = response.index("csrftoken")
  # Find csrftoken in body.
  csrf2index = response.index("csrfmiddlewaretoken")
  # CSRFToken pulled from header of initial GET request.
  csrfToken1 = response[csrf1index + 10 : csrf1index + 74]
  # CSRFToken pulled from the hidden csrfmiddlewaretoken field in body of initial GET request.
  csrfToken2 = response[csrf2index + 28 : csrf2index + 92]

  # Forge the POST request with the current csrf token to sign into fakebook.
  usernamePass = "username=" + sys.argv[1] + "&password=" + sys.argv[2]
  payload = "" + usernamePass + "&csrfmiddlewaretoken=" + csrfToken2 + "&next=\r\n"
  POST_REQUEST = (f"POST /accounts/login/ HTTP/1.1\r\n"
  "Host: webcrawler-site.ccs.neu.edu\r\n"
  "Accept-Encoding: gzip, deflate\r\n"
  "Referer: http://webcrawler-site.ccs.neu.edu/accounts/login/\r\n"
  "Content-Type: application/x-www-form-urlencoded\r\n"
  "Content-Length: " + str(len(payload)) + "\r\n"
  "Origin: http://webcrawler-site.ccs.neu.edu\r\n"
  "Connection: keep-alive\r\n"
  "Cookie: csrftoken=" + csrfToken1 + "\r\n"
  "Upgrade-Insecure-Requests: 1\r\n"
  "\r\n"
  + payload + "\r\n")
  POST_REQUEST = POST_REQUEST.encode('ascii')

  postResponse = ""
  # Send the Post request.
  while "csrftoken" not in postResponse or "sessionid" not in postResponse:
    connection.sendall(POST_REQUEST)
    try:
      postResponse = bytes.decode(connection.recv(8192))
    except:
      postResponse = ""
  # Give me the cookie and the sessionID.
  cookieIndex = postResponse.index("csrftoken")
  sessionIDIndex = postResponse.index("sessionid")
  cookie = postResponse[cookieIndex + 10 : cookieIndex + 74]
  sessionID = postResponse[sessionIDIndex + 10 : sessionIDIndex + 42]
  return cookie, sessionID, connection
 
# Iterate through a built up list of urls in a queue and update them in a dict when they get searched.
def iterate_through(connection, html, cookie, sessionID):
    global pages_seen
    global body_chunk1
    global body_chunked_huh
    global prev_size
    # Do the initial html functions on our home page.
    grab_links(html)
    find_flags(html)
    curr_conn = connection

    # While we still have flags to find and links to explore in our queue.
    while len(queue) > 0:
      pages_seen += 1
      curr_in_queue = queue[0]
      # Send GET request to pull down the html for the current url.
      currhtml = sendGET(curr_conn, curr_in_queue, False, cookie, sessionID)      
      if len(currhtml) == 0 or "HTTP/1.1 400" in currhtml: # add the 500 error case in this if statement.
          try:
            curr_conn.close()
          except:
            print("Already Closed")
          time.sleep(1)
          curr_conn = connect()
          currhtml = sendGET(curr_conn, curr_in_queue, False, cookie, sessionID)
         # print("After restart: ", currhtml)
      # Search the new html page for new links.
      if body_chunked_huh:
          diff = prev_size - (len(body_chunk1) + 3)
          body_chunk1 += currhtml
          grab_links(body_chunk1)
          find_flags(body_chunk1)
          prev_size = 0
          body_chunked_huh = False
          body_chunk1 = ""
      buff = ""
      local_body_chunked_huh = False
      if "Content-Length:" in currhtml: 
        cli = currhtml.index("Content-Length:")
        ii = cli + 15
        if '\n' in currhtml:
          while currhtml[ii] != '\n':
            buff += currhtml[ii]
            ii += 1
      if len(buff) > 1 and "<!D" in currhtml:
          size = int(buff)
          body_index = currhtml.index("<!D")
          body = currhtml[body_index:]
          if size != len(body) + 3:
              redo = queue[0]
              queue.append(redo)
              prev_size = size
              local_body_chunked_huh = True
              body_chunk1 += body
              #grab_links(body + '\n')
      #if not body_chunked_huh:
      grab_links(currhtml)
      body_chunked_huh = local_body_chunked_huh
      # Search the new html page for flags.
      find_flags(currhtml)
      # Update our data structures.
      links_to_parse[curr_in_queue] = True
      queue.pop(0)

# Main method for this webcrawler. 
# Get cookie, start searching for links and flags, maintain queue of valid links left to explore, print out found flags.
def main():
  global pages_seen
  cookie, sessionID, connection = login()
  initialHTML = ""
  while "/fakebook/" not in initialHTML:
      initialHTML = sendGET(connection, "/fakebook/", False, cookie, sessionID)
  iterate_through(connection, initialHTML, cookie, sessionID)
  print(pages_seen)

if __name__ == "__main__":
  # Command line specification enforcing.
  if not(len(sys.argv) == 3):
    sys.exit("Usage: ./webcrawler [username] [password]")
  # Call the main method.
  main()
